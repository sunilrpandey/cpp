//Thread notes.

####  Thread init
- include <thread>
- init thread object with any global function/function object/class member function/packaged_task/class instance
- use join function to get thread completed and detatch to make thread independent. as join/detatch can be called only once use if(t1.joinable()) before calling join/detatch.

###Share resource
- When you have multiple thread in a program they might need to share resources e.g. log file/task queue etc.
- for this one can use std::mutex and lock/unlock it using its lock/unlock functions. 
== functioToaccessSharedResource(){
	std::mutex mu;
	mu.lock();
	-- implement functionality 
	--
	--
	mu.unlock()
	what happen if implmentation crashes, mutex wont be unlocked so its necessary to write wrapper around mutex.
	
- std provides std::lock_guard //locker(mu)// for this purpose
- if you want deferred lock/unlock use unique_lock
== std::unique_lock uLocker(mu,std::deferred) and use lock/unlock method of unique_lock.

// try to encpsulate mutex in resource which you are gonan shared, e.g. log file or stack.
class Logfile
{
	ofstream f;
	mutex mu;
	std::once_flag flat_;
	public:
	Logfile(){
		if(!f.is_open()){
			unique_lock locker(mu);
			if(!f.is_open()){
				f.open("log.txt");
				}
			}
		}
	shared_print(string str) {// if lazy initialization
		call_once(flag_,[&](){f.open("log.txt"}) // an alternative to mutex if task it to  be called once
		f << str;
		}
	}
//Recursive_mutex
Ref : https://baptiste-wicht.com/posts/2012/04/c11-concurrency-tutorial-advanced-locking-and-condition-variables.html
github ref: https://github.com/wichtounet/articles/tree/master/src/threads
Suppose we have mul/div funciton and one which does both
mul(int x)
std::lock_guard<std::recursive_mutex> lock(mutex);
i *= x
}
div(int x)
std::lock_guard<std::recursive_mutex> lock(mutex);
i /= x
}

void MathUtil::both(int x, int y){
        std::lock_guard<std::recursive_mutex> lock(mutex);
        mul(x);
        div(y);
    }
	
if both of MathUtil is called it will lock and call mul where this mutex will be called again.. which would not have been possible it mutex had been std::mutex, here mutex can be locked several times
//std::timed_mutex should be used when you want to other util function such as timedMutex.try_lock_for(timeout;)
set time for timeout or sleepout using std::chrono::milliseconds timeout(1001)
ConditionVariable
A condition variable manages a list of threads waiting until another thread notify them. Each thread that wants to wait on the condition variable has to acquire a lock first. The lock is then released when the thread starts to wait on the condition and the lock is acquired again when the thread is awakened.
SharedBuffer Code for conditional variable

struct BoundedBuffer {
    int* buffer;
    int capacity;

    int front;
    int rear;
    int count;

    std::mutex lock;

    std::condition_variable not_full;
    std::condition_variable not_empty;

    BoundedBuffer(int capacity) : capacity(capacity), front(0), rear(0), count(0) {
        buffer = new int[capacity];
    }

    ~BoundedBuffer(){
        delete[] buffer;
    }

    void deposit(int data){
        std::unique_lock<std::mutex> l(lock);

        not_full.wait(l, [this](){return count != capacity; });

        buffer[rear] = data;
        rear = (rear + 1) % capacity;
        ++count;

        l.unlock();
        not_empty.notify_one();
    }

    int fetch(){
        std::unique_lock<std::mutex> l(lock);

        not_empty.wait(l, [this](){return count != 0; });

        int result = buffer[front];
        front = (front + 1) % capacity;
        --count;

        l.unlock();
        not_full.notify_one();

        return result;
    }
};

The mutexes are managed by a std::unique_lock. It is a wrapper to manage a lock. This is necessary to be used with the condition variables. To wake up a thread that is waiting on a condition variable, the notify_one() function is used. The unlock before the notify_one is not totally necessary. If you omit it, it will be done automatically by destructor of the unique_lock. But, it is then possible that the notify_one() call will wake up a waiting thread that will then directly block again since the lock itself is still locked by the notifier thread. Therefore, if you do it before, the notified thread should be able to get the lock directly. Therefore, it's a slight optimization, but it won't make a lot of differences. The wait function is a bit special. It takes as the first argument the unique lock and a the second one a predicate. The predicate must return false when the waiting must be continued (it is equivalent to while(!pred()){cv.wait(l);}). The rest of the example has nothing special.

We can use this structure to fix multiple consumers / multiple producers problem. This problem is very common in concurrent programming. Several threads (consumers) are waiting from data produced by another several threads (producers). Here is an example with several threads using the structure:

void consumer(int id, BoundedBuffer& buffer){
    for(int i = 0; i < 50; ++i){
        int value = buffer.fetch();
        std::cout << "Consumer " << id << " fetched " << value << std::endl;
        std::this_thread::sleep_for(std::chrono::milliseconds(250));
    }
}

void producer(int id, BoundedBuffer& buffer){
    for(int i = 0; i < 75; ++i){
        buffer.deposit(i);
        std::cout << "Produced " << id << " produced " << i << std::endl;
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

int main(){
    BoundedBuffer buffer(200);

    std::thread c1(consumer, 0, std::ref(buffer));
    std::thread c2(consumer, 1, std::ref(buffer));
    std::thread c3(consumer, 2, std::ref(buffer));
    std::thread p1(producer, 0, std::ref(buffer));
    std::thread p2(producer, 1, std::ref(buffer));

    c1.join();
    c2.join();
    c3.join();
    p1.join();
    p2.join();

    return 0;
}

// std::atomic
The C++11 Concurrency Library introduces Atomic Types as a template class: std::atomic. You can use any Type you want with that template and the operations on that variable will be atomic and so thread-safe. It has to be taken into account that it is up to the library implementation to choose which syncronization mechanism is used to make the operations on that type atomic. On standard platforms for integral types like int, long, float, ... it will be some lock-free technique. If you want to make a big type (let's saw 2MB storage), you can use std::atomic as well, but mutexes will be used. In this case, there will be no performance advantage.

std::atomic is specialized for all integral types to provide member functions specific to integral (like operators ++, --, fetch_add, fetch_sub, ...).
//=======================================================================
// Copyright (c) 2014 Baptiste Wicht
// Distributed under the terms of the MIT License.
// (See accompanying file LICENSE or copy at
//  http://opensource.org/licenses/MIT)
//=======================================================================

Ref: https://github.com/wichtounet/articles/blob/master/src/threads/part4/AtomicCounter.cpp
Ref: https://www.classes.cs.uchicago.edu/archive/2013/spring/12300-1/labs/lab6/
#include <thread>
#include <atomic>
#include <iostream>
#include <vector>

struct AtomicCounter {
    std::atomic<int> value;

    AtomicCounter() : value(0) {}

    void increment(){
        ++value;
    }

    void decrement(){
        --value;
    }

    int get(){
        return value.load();
    }
};

int main(){
    AtomicCounter counter;

    std::vector<std::thread> threads;
    for(int i = 0; i < 10; ++i){
        threads.push_back(std::thread([&counter](){
            for(int i = 0; i < 500; ++i){
                counter.increment();
            }
        }));
    }

    for(auto& thread : threads){
        thread.join();
    }

    std::cout << counter.get() << std::endl;

    return 0;
}

//std:;async
-  what if you want return value from thread, it returns std::future<T>
- you can also defer thread call (asyncb) by passing std::launch::defered, instead of std::launch::async
- thread will be called only when you call retValue.get()
- you can also pass std:;future to thread functiona and get value passed using get fuction in thread function
- in fact you can pass futute that is yet to collected from promise
std::promise p;
std::future f = p.get_future();
thread_fun(f).
std::future<int> retValue = std::async(std::launch::async,thread_fun(f));
p->set_value(9);
factValue = retValue.get();
//if you want to use same future value in different thread, since generally you can pass this 
// future value using ref/move
std::shared_future sf = fut.share()

class A {
public:
	string note;
	void f(int x, char c) { }
	long g(double x) { note = "changed"; return 0;}
	int operator()(int N) { return 0;}
};
A a;

int main() {
	a.note = "Original"; 
	std::future<int> fu3 = std::async(A(), 4);    // A tmpA;  tmpA is moved to async(); create a task/thread with tmpA(4);
	std::future<int> fu4 = std::async(a, 7);    
	std::future<int> fu4 = std::async(std::ref(a), 7); // a(7);  Must use reference wrapper
	std::future<int> fu5 = std::async(&a, 7); // Won't compile

	std::future<void> fu1 = std::async(&A::f, a, 56, 'z'); // A copy of a invokes f(56, 'z')
	std::future<long> fu2 = std::async(&A::g, &a, 5.6);    // a.g(5.6);  a is passed by reference
		// note: the parameter of the invocable are always passed by value, but the invokeable itself can be passed by ref.
	cout << a.note << endl;
	return 0;
}
/* packaged_task exmaple */
std::packaged_task along with std::bind can be put in a queue send across thread.

std::mutex mu;
std::deque<std::packaged_task<int()> > task_q;

int factorial(int N) {
	int res = 1;
	for (int i=N; i>1; i--)
		res *= i;

	return res;
}

void thread_1() {
	for (int i=0; i<10000; i++) {
		std::packaged_task<int()> t;
		{
			std::lock_guard<std::mutex> locker(mu);
			if (task_q.empty()) 
				continue;
			t = std::move(task_q.front());
			task_q.pop_front();
		}
		t();
	}
}

int main() {
	std::thread th(thread_1);

	std::packaged_task<int()> t(bind(factorial, 6));  
	std::future<int> ret = t.get_future();
	std::packaged_task<int()> t2(bind(factorial, 9));
	std::future<int> ret2 = t2.get_future();
	{
		std::lock_guard<std::mutex> locker(mu);
		task_q.push_back(std::move(t));
		task_q.push_back(std::move(t2));
	}
	cout << "I see: " << ret.get() << endl;
	cout << "I see: " << ret2.get() << endl;

	th.join();
	return 0;
}

 